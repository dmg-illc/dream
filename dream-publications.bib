
@inproceedings{giulianelli-etal-2023,
    title = "Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis",
    author = "Giulianelli, Mario  and
      Luden, Iris  and
      Fern{\'a}ndez, Raquel  and
      Kutuzov, Andrey",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.176",
    pages = "3130--3148",
    abstract = "We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations.Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label. We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users {---} historical linguists, lexicographers, or social scientists {---} to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the {`}definitions as representations{'} paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements, making them a new promising type of lexical representation for NLP.",
}

@inproceedings{takmaz-etal-2023-speaking,
    title = "Speaking the Language of Your Listener: Audience-Aware Adaptation via Plug-and-Play Theory of Mind",
    author = "Takmaz, Ece  and
      Brandizzi, Nicolo{'}  and
      Giulianelli, Mario  and
      Pezzelle, Sandro  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.258",
    pages = "4198--4217",
    abstract = "Dialogue participants may have varying levels of knowledge about the topic under discussion. In such cases, it is essential for speakers to adapt their utterances by taking their audience into account. Yet, it is an open question how such adaptation can be modelled in computational agents. In this paper, we model a visually grounded referential game between a knowledgeable speaker and a listener with more limited visual and linguistic experience. Inspired by psycholinguistic theories, we endow our speaker with the ability to adapt its referring expressions via a simulation module that monitors the effectiveness of planned utterances from the listener{'}s perspective. We propose an adaptation mechanism building on plug-and-play approaches to controlled language generation, where utterance generation is steered on the fly by the simulator without finetuning the speaker{'}s underlying language model. Our results and analyses show that our approach is effective: the speaker{'}s utterances become closer to the listener{'}s domain of expertise, which leads to higher communicative success.",
}

@article{Pezzelle-Fernandez-2023-cogsci,
author = {Pezzelle, Sandro and Fern{\'a}ndez, Raquel},
title = {Semantic Adaptation to the Interpretation of Gradable Adjectives via Active Linguistic Interaction},
journal = {Cognitive Science},
volume = {47},
number = {2},
pages = {e13248},
keywords = {Gradable adjectives, Semantic adaptation, Vagueness, Questions, Linguistic interaction, Visual grounding},
doi = {https://doi.org/10.1111/cogs.13248},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13248},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13248},
abstract = {Abstract When communicating, people adapt their linguistic representations to those of their interlocutors. Previous studies have shown that this also occurs at the semantic level for vague and context-dependent terms such as quantifiers and uncertainty expressions. However, work to date has mostly focused on passive exposure to a given speaker's interpretation, without considering the possible role of active linguistic interaction. In this study, we focus on gradable adjectives big and small and develop a novel experimental paradigm that allows participants to ask clarification questions to figure out their interlocutor's interpretation. We find that, when in doubt, speakers do resort to this strategy, despite its inherent cognitive cost, and that doing so results in higher semantic alignment measured in terms of communicative success. While not all question–answer pairs are equally informative, we show that speakers become better questioners as the interaction progresses. Yet, the higher semantic alignment observed when speakers are able to ask questions does not increase over time. This suggests that conversational interaction's key advantage may be to boost coordination without committing to long-term semantic updates. Our findings shed new light on the mechanisms used by speakers to achieve semantic alignment and on how language is shaped by communication.},
year = {2023}
}

@inproceedings{jansen-etal-2022-controllable,
    title = "Controllable Text Generation for All Ages: Evaluating a Plug-and-Play Approach to Age-Adapted Dialogue",
    author = "Jansen, Lennert  and
      Laichter, {\v{S}}t{\v{e}}p{\'a}n Lars  and
      Sinclair, Arabella  and
      van der Goot, Margot  and
      Fern{\'a}ndez, Raquel  and
      Pezzelle, Sandro",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.14",
    pages = "172--188",
    abstract = "To be trusted and perceived as natural and coherent, conversational systems must adapt to the language of their users. While personalized dialogue is a promising direction, controlling generation for fine-grained language features remains a challenge in this approach. A recent line of research showed the effectiveness of leveraging pre-trained language models toward adapting to a text{'}s topic or sentiment. In this study, we build on these approaches and focus on a higher-level dimension of language variation: speakers{'} age. We frame the task as a dialogue response generation, and test methods based on bag-of-words (BoW) and neural discriminators (Disc) to condition the output of GPT-2 and DialoGPT without altering the parameters of the language models. We show that Disc models achieve a higher degree of detectable control than BoW models based on automatic evaluation. In contrast, humans can partially detect age differences in BoW but not Disc responses. Since BoW responses are deemed better than Disc ones by humans, simple controllable methods thus appear to be a better tradeoff between adaptation and language quality. Our work confirms the challenges of adapting to higher-level dimensions of language variation. Moreover, it highlights the need to evaluate natural language generation thoroughly.",
}

@inproceedings{baan-etal-2022-emnlp,
    title = "Stop Measuring Calibration When Humans Disagree",
    author = "Baan, Joris  and
      Aziz, Wilker  and
      Plank, Barbara  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.124",
    pages = "1892--1915",
    abstract = "Calibration is a popular framework to evaluate whether a classifier knows when it does not know - i.e., its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class. Recently, calibration to human majority has been measured on tasks where humans inherently disagree about which class applies. We show that measuring calibration to human majority given inherent disagreements is theoretically problematic, demonstrate this empirically on the ChaosNLI dataset, and derive several instance-level measures of calibration that capture key statistical properties of human judgements - including class frequency, ranking and entropy.",
}

@inproceedings{giulianelli-2022-emnlp,
    title = "Towards Pragmatic Production Strategies for Natural Language Generation Tasks",
    author = "Giulianelli, Mario",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.544",
    pages = "7978--7984",
    abstract = "This position paper proposes a conceptual framework for the design of Natural Language Generation (NLG) systems that follow efficient and effective production strategies in order to achieve complex communicative goals. In this general framework, efficiency is characterised as the parsimonious regulation of production and comprehension costs while effectiveness is measured with respect to task-oriented and contextually grounded communicative goals. We provide concrete suggestions for the estimation of goals, costs, and utility via modern statistical methods, demonstrating applications of our framework to the classic pragmatic task of visually grounded referential games and to abstractive text summarisation, two popular generation tasks with real-world applications. In sum, we advocate for the development of NLG systems that learn to make pragmatic production decisions from experience, by reasoning about goals, costs, and utility in a human-like way.",
}


@inproceedings{giulianelli-etal-2022-construction,
    title = "Construction Repetition Reduces Information Rate in Dialogue",
    author = "Giulianelli, Mario  and
      Sinclair, Arabella  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing",
    month = nov,
    year = "2022",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.51",
    pages = "665--682",
    abstract = "Speakers repeat constructions frequently in dialogue. Due to their peculiar information-theoretic properties, repetitions can be thought of as a strategy for cost-effective communication. In this study, we focus on the repetition of lexicalised constructions{---}i.e., recurring multi-word units{---}in English open-domain spoken dialogues. We hypothesise that speakers use construction repetition to mitigate information rate, leading to an overall decrease in utterance information content over the course of a dialogue. We conduct a quantitative analysis, measuring the information content of constructions and that of their containing utterances, estimating information content with an adaptive neural language model. We observe that construction usage lowers the information content of utterances. This facilitating effect (i) increases throughout dialogues, (ii) is boosted by repetition, (iii) grows as a function of repetition frequency and density, and (iv) is stronger for repetitions of referential constructions.",
}

@article{sinclair-fernandez-system-2022,
title = {Alignment of code switching varies with proficiency in second language learning dialogue},
journal = {System. Special Issue on Linguistic alignment in Second Language Acquisition: occurrences, learning effects, and beyond},
year = {2022},
issn = {0346-251X},
publisher = {Elsevier},
doi = {https://doi.org/10.1016/j.system.2022.102952},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X22002342},
author = {Arabella J. Sinclair and Raquel Fern\'andez},
abstract = {Speakers in dialogue tend to adopt the language patterns of the other, aligning their language to their interlocutor. This can happen at many levels of communication, including the tendency to code switch (CS), or change to another language. Alignment has often been considered the result of an unconscious automatic process that facilitates speakers' mutual understanding. In dialogues with a second language (L2) learner, alignment is constrained by the proficiency of the learner, and additional non-automatic processes will be at play, namely the individual pedagogical goals of learner and tutor. In this study, we investigate alignment in dialogues between Spanish/Catalan learners of English and their tutors. We analyse CS incidence, whether code switching can be explained as automatic alignment between speakers, and whether this is independent of other, non-automatic factors related to speakers’ goals. We find that alignment of code switching is present, varies with learner proficiency, and that code switching can additionally be triggered by lexical overlap and turn taking asymmetry, which we attribute to conscious pedagogical choices on the part of both tutor, at lower levels, and learner, at higher levels of student proficiency.}
}

@Article{sinclair-etal-2022-tacl,
  author = 	 "Arabella Sinclair and Jaap Jumelet and Willem Zuidema and Raquel Fern\'andez",
  title = 	 "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
  journal = 	 "Transactions of the Association for Computational Linguistics (TACL)",
  year = "2022",
  url = "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00504/113019/Structural-Persistence-in-Language-Models-Priming",
  abstract = "We investigate the extent to which modern, neural language models are susceptible to structural priming,
  the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up  sentence.
  We explore how priming can be used to study the potential of these models to learn abstract structural information,
  which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce
  a novel metric and release PRIME-LM, a large corpus where we control for various linguistic factors which interact with priming strength.
  We find that Transformer models indeed show evidence of structural priming, but also that the generalisations they learned are to
  some extent modulated by semantic information. Our experiments also show that the representations acquired by the models
  may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information.
  More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities
  of language models and opens the door to future priming-based investigations that probe the model's internal states."
}

@inproceedings{ryb-etal-2022-analog,
    title = "{A}na{L}og: Testing Analytical and Deductive Logic Learnability in Language Models",
    author = "Ryb, Samuel  and
      Giulianelli, Mario  and
      Sinclair, Arabella  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.starsem-1.5",
    pages = "55--68",
    abstract = "We investigate the extent to which pre-trained language models acquire analytical and deductive logical reasoning
    capabilities as a side effect of learning word prediction. We present AnaLog, a natural language inference task designed to
    probe models for these capabilities, controlling for different invalid heuristics the models may adopt instead of learning the
    desired generalisations. We test four languagemodels on AnaLog, finding that they have all learned, to a different extent,
    to encode information that is predictive of entailment beyond shallow heuristics such as lexical overlap and grammaticality.
    We closely analyse the best performing language model and show that while it performs more consistently than other language
    models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the
    realisation of logical statements."
}

@inproceedings{takmaz-etal-2022-cmcl,
    title = "Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via {CLIP}",
    author = "Takmaz, Ece  and Pezzelle, Sandro  and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.cmcl-1.4",
    pages = "36--42",
    abstract = "In this work, we use a transformer-based pre-trained multimodal model, CLIP,
    to shed light on the mechanisms employed by human speakers when referring to visual entities.
    In particular, we use CLIP to quantify the degree of descriptiveness (how well an utterance
    describes an image in isolation) and discriminativeness (to what extent an utterance is effective
    in picking out a single image among similar images) of human referring utterances within multimodal
    dialogues. Overall, our results show that utterances become less descriptive over time while their
    discriminativeness remains unchanged. Through analysis, we propose that this trend could be due to
    participants relying on the previous mentions in the dialogue history, as well as being able to
    distill the most discriminative information from the visual context. In general, our study opens
    up the possibility of using this and similar models to quantify patterns in human data and shed
    light on the underlying cognitive mechanisms."
}

@inproceedings{takmaz-2022-team,
    title = "Team {DMG} at {CMCL} 2022 Shared Task: Transformer Adapters for the Multi- and Cross-Lingual Prediction of Human Reading Behavior",
    author = "Takmaz, Ece",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = may,
    year = "2022",
    bibbase_note = {Best Shared Task Paper Award},
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.cmcl-1.16",
    pages = "136--144",
    abstract = "In this paper, we present the details of our approaches that attained the second place
    in the shared task of the ACL 2022 Cognitive Modeling and Computational Linguistics Workshop.
    The shared task is focused on multi- and cross-lingual prediction of eye movement features in
    human reading behavior, which could provide valuable information regarding language processing.
    To this end, we train {`}adapters{'} inserted into the layers of frozen transformer-based pretrained
    language models. We find that multilingual models equipped with adapters perform well in predicting
    eye-tracking features. Our results suggest that utilizing language- and task-specific adapters is
    beneficial and translating test sets into similar languages that exist in the training set could
    help with zero-shot transferability in the prediction of human reading behavior."
}

@inproceedings{giulianelli-etal-2022-fire,
    title = "Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change",
    author = "Giulianelli, Mario  and Kutuzov, Andrey  and Pivovarova, Lidia",
    booktitle = "Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.lchange-1.6",
    pages = "54--67",
    abstract = "Morphological and syntactic changes in word usage {---} as captured, e.g., by
    grammatical profiles {---} have been shown to be good predictors of a word{'}s meaning change.
    In this work, we explore whether large pre-trained contextualised language models, a common tool
    for lexical semantic change detection, are sensitive to such morphosyntactic changes. To this end,
    we first compare the performance of grammatical profiles against that of a multilingual neural
    language model (XLM-R) on 10 datasets, covering 7 languages, and then combine the two approaches
    in ensembles to assess their complementarity. Our results show that ensembling grammatical
    profiles with XLM-R improves semantic change detection performance for most datasets and languages.
    This indicates that language models do not fully cover the fine-grained morphological and syntactic
    signals that are explicitly represented in grammatical profiles. An interesting exception are the
    test sets where the time spans under analysis are much longer than the time gap between them
    (for example, century-long spans with a one-year gap between them). Morphosyntactic change is
    slow so grammatical profiles do not detect in such cases. In contrast, language models, thanks
    to their access to lexical information, are able to detect fast topical changes."
}


@Article{pezzelle-etal-2021-tacl,
  author = 	 "Sandro Pezzelle and Ece Takmaz and Raquel Fern\'andez",
  title = 	 "Word Representation Learning in Multimodal Pre-Trained Transformers: An Intrinsic Evaluation",
  journal = 	 "Transactions of the Association for Computational Linguistics (TACL)",
  year = "2021",
  url = "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00443/1979754/tacl_a_00443.pdf",
  url_github = "https://github.com/sandropezzelle/multimodal-evaluation",
  abstract = "This study carries out a systematic /intrinsic/ evaluation of the semantic representations
	learned by state-of-the-art pre-trained multimodal Transformers.
	These representations are claimed to be task-agnostic and shown to help on many downstream language-and-vision tasks.
	However, the extent to which they align with human semantic intuitions remains unclear.
	We experiment with various models and obtain /static/ word representations from the /contextualized/ ones they learn.
	We then evaluate them against the semantic judgements provided by human speakers. In line with previous evidence,
	we observe a generalized advantage of multimodal representations over language-only ones on concrete word pairs,
  but not on abstract ones. On the one hand, this confirms the effectiveness of these models to
	align language and vision, which results in better semantic representations for concepts that are /grounded/ in images.
	On the other hand, models are shown to follow different representation learning patterns, which
	sheds some light on /how/ and /when/ they perform multimodal integration."
}


@inproceedings{jansen-etal-2021-detecting,
    title = "Detecting Age-Related Linguistic Patterns in Dialogue: Toward Adaptive Conversational Systems",
    author = "Jansen, Lennert and Sinclair, Arabella and van der Goot, Margot J. and Fern{\'a}ndez, Raquel and Pezzelle, Sandro",
    booktitle = "Proceedings of the Eighth Italian Conference on Computational Linguistics (CLiC-it)",
    year = "2021",
    url = "http://ceur-ws.org/Vol-3033/paper47.pdf",
    url_github = "https://github.com/ lennertjansen/detecting-age-in-dialogue",
    abstract = "This work explores an important dimension of variation in the language used by dialogue
    participants: their age. While previous work showed differences at various linguistic levels between
    age groups when experimenting with written discourse data (e.g., blog posts), previous work on dialogue
    has largely been limited to acoustic information related to voice and prosody. Detecting fine-grained
    linguistic properties of human dialogues is of crucial importance for developing AI- based conversational
    systems which are able to adapt to their human interlocutors. We therefore investigate whether, and to
    what extent, current text-based NLP models can detect such linguistic differences, and what the features
    driving their predictions are. We show that models achieve a fairly good performance on age- group prediction,
    though the task appears to be more challenging compared to discourse. Through in-depth analysis of the
    best models’ errors and the most predictive cues, we show that, in dialogue, differences among age groups
    mostly concern stylistic and lexical choices. We believe these findings can inform future work on developing
    controlled generation models for adaptive conversational systems."
}

@inproceedings{giulianelli-etal-2021-information,
    title = "Is Information Density Uniform in Task-Oriented Dialogues?",
    author = "Giulianelli, Mario and Sinclair, Arabella and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.emnlp-main.652/",
    url_github = "https://github.com/dmg-illc/uid-dialogue",
    abstract = "The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations
    in the density of the information transmitted. In this paper, we test whether, and within which contextual units this
    principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues
    where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following.
    Our study underlines the importance of identifying the relevant contextual components, showing that information content
    increases particularly within topically and referentially related contextual units."
}

@inproceedings{giulianelli-fernandez-2021-analysing,
    title = "Analysing Human Strategies of Information Transmission as a Function of Discourse Context",
    author = "Giulianelli, Mario and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.conll-1.50/",
    url_github = "https://github.com/dmg-illc/uid-dialogue",
    abstract = "Speakers are thought to use rational information transmission strategies for efficient communication.
    Previous work analysing these strategies in sentence production has failed to take into account how the information
    content of sentences varies as a function of the available discourse context.
    In this study, we estimate sentence information content within discourse context.
    We find that speakers transmit information at a stable rate---i.e., rationally---in English newspaper articles
    but that this rate decreases in spoken open domain and written task-oriented dialogues.
    We also observe that speakers' choices are not oriented towards local uniformity of information,
    which is another hypothesised rational strategy. We suggest that a more faithful model of communication
    should explicitly include production costs and goal-oriented rewards."
}

@inproceedings{sinclair-fernandez-2021-construction,
    title = "Construction coordination in first and second language acquisition",
    author = "Sinclair, Arabella  and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 25th Workshop on the Semantics and Pragmatics of Dialogue",
    year = "2021",
    address = "Potsdam, Germany",
    publisher = "SEMDIAL",
    url = "http://semdial.org/anthology/Z21-Sinclair_semdial_0018.pdf",
    abstract = "Repetition of linguistic forms is a pervasive coordination mechanism in interactive language use.
    In this paper, we investigate patterns of cross-participant repetition in dialogues where participants have
    different levels of linguistic ability. Achieving a better understanding of these patterns can not only shed
    light on how humans coordinate in conversation, but may also contribute to developing more natural and effective
    dialogue agents in education contexts related to language learning. Our approach is novel in several respects:
    We focus on multi-word constructions at the lexical and morphosyntactic level, consider both first and second
    acquisition dialogue, and contrast these setups with adult native conversation. The results of our study show
    that language acquisition scenarios are characterised by richer inventories of shared constructions but lower
    usage rates than fluent adult dialogues, and that shared construction use evolves as the linguistic ability
    of the learners increases, arguably leading to a process of routinisation."
}

@inproceedings{giulianelli-etal-2021-profiling,
    title = "Grammatical Profiling for Semantic Change Detection",
    author = "Giulianelli, Mario and Kutuzov, Andrey and Pivovarova, Lidia",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.conll-1.33/",
    url_GitHub = "https://github.com/glnmario/semchange-profiling",
    abstract = "Semantics, morphology and syntax are strongly interdependent. However, the majority of computational methods for semantic change detection use distributional word representations which encode mostly semantics. We investigate an alternative method, grammatical profiling, based entirely on changes in the morphosyntactic behaviour of words. We demonstrate that it can be used for semantic change detection and even outperforms some distributional semantic methods. We present an in-depth qualitative and quantitative analysis of the predictions made by our grammatical profiling system, showing that they are plausible and interpretable."
}

@inproceedings{parfenova-etal-2021-probing,
    title = "Probing Cross-Modal Representations in Multi-Step Relational Reasoning",
    author = "Parfenova, Iuliia  and
      Elliott, Desmond  and
      Fern{\'a}ndez, Raquel  and
      Pezzelle, Sandro",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.16.pdf",
    doi = "10.18653/v1/2021.repl4nlp-1.16",
    url_GitHub = "https://github.com/jig-san/multi-step-size-reasoning",
    pages = "152--162",
    abstract = "We investigate the representations learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.",
}

@inproceedings{noble-etal-2021-semantic,
    title = "Semantic shift in social networks",
    author = "Noble, Bill  and
      Sayeed, Asad  and
      Fern{\'a}ndez, Raquel  and
      Larsson, Staffan",
    booktitle = "Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.starsem-1.3.pdf",
    doi = "10.18653/v1/2021.starsem-1.3",
    url_GitHub = "https://github.com/GU-CLASP/semantic-shift-in-social-networks",
    pages = "26--37",
    abstract = "Just as the meaning of words is tied to the communities in which they are used, so too is semantic change. But how does lexical semantic change manifest differently across different communities? In this work, we investigate the relationship between community structure and semantic change in 45 communities from the social media website Reddit. We use distributional methods to quantify lexical semantic change and induce a social network on communities, based on interactions between members. We explore the relationship between semantic change and the clustering coefficient of a community{'}s social network graph, as well as community size and stability. While none of these factors are found to be significant on their own, we report a significant effect of their three-way interaction. We also report on significant word-level effects of frequency and change in frequency, which replicate previous findings.",
}

@inproceedings{takmaz-etal-2020-refer,
    title = "{R}efer, {R}euse, {R}educe: {G}enerating {S}ubsequent {R}eferences in {V}isual and {C}onversational {C}ontexts",
    author = "Takmaz, Ece  and
      Giulianelli, Mario  and
      Pezzelle, Sandro  and
      Sinclair, Arabella  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.353.pdf",
    url_dataset = "https://dmg-photobook.github.io",
    doi = "10.18653/v1/2020.emnlp-main.353",
    pages = "4350--4368",
    abstract = "Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.",
}

@inproceedings{takmaz-etal-2020-generating,
    title = "{G}enerating {I}mage {D}escriptions via {S}equential {C}ross-{M}odal {A}lignment {G}uided by {H}uman {G}aze",
    author = "Takmaz, Ece  and
      Pezzelle, Sandro  and
      Beinborn, Lisa  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.377.pdf",
    url_GitHub = "https://github.com/dmg-illc/didec-seq-gen",
    doi = "10.18653/v1/2020.emnlp-main.353",
    pages = "4664--4677",
    abstract = "When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural{---}particularly when gaze is encoded with a dedicated recurrent component.",
}


@inproceedings{giulianelli-etal-2020-analysing,
    title = "Analysing Lexical Semantic Change with Contextualised Word Representations",
    author = "Giulianelli, Mario  and
      Del Tredici, Marco  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.365.pdf",
    url_Dataset = {https://doi.org/10.5281/zenodo.3773250},
    doi = "10.18653/v1/2020.acl-main.365",
    pages = "3960--3973",
    abstract = "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.",
}

@inproceedings{kutuzov-giulianelli-2020-change,
    title = "{U}i{O}-{U}v{A} at {S}em{E}val-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection",
    author = "Kutuzov, Andrey  and
      Giulianelli, Mario",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.14",
    url_GitHub = "https://github.com/akutuzov/semeval2020",
    pages = "126--134",
    abstract = "We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular algorithm depends on the distribution of gold scores in the test set.",
}


@inproceedings{gualdoni-etal:2020:clicit,
  author={Eleonora Gualdoni and Raffaella Bernardi and Raquel Fern\'andez and Sandro Pezzelle},
  title={Grounded and Ungrounded Referring Expressions in Human Dialogues: Language Mirrors Different Grounding Conditions},
  booktitle={Proceedings of the 7th Italian Conference on Computational Linguistics (CLiC-it)},
  year={2020},
  url={http://ceur-ws.org/Vol-2769/paper_38.pdf}
}
