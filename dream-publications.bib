@inproceedings{jansen-etal-2022-gem,
    title = "Controllable Text Generation for All Ages: Evaluating a Plug-and-Play Approach to Age-Adapted Dialogue",
    author = "Lennert Jansen and \v{S}t\v{e}p\'{a}n Lars Laichter and Arabella Sinclair and Margot J. van der Goot and Raquel Fern{\'a}ndez and Sandro Pezzelle",
    booktitle = "Proceedings of the Second Workshop on Generation, Evaluation and Metrics (GEM)",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    note = "To appear"
}

@inproceedings{giulianelli-2022-emnlp,
    title = "Towards Pragmatic Production Strategies for Natural Language Generation",
    author = "Mario Giulianelli",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022)",
    year = "2022",
    url = "https://arxiv.org/abs/2210.12828",
    note = "To appear"
}

@inproceedings{giulianelli-etal-2022-aacl,
    title = "{Construction Repetition Reduces Information Rate in Dialogue}",
    author = "Mario Giulianelli and Arabella Sinclair and Raquel Fern\'andez",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (AACL-IJCNLP 2022)",
    year = "2022",
    url = "https://arxiv.org/abs/2210.08321",
    note = "To appear",
    abstract = "Speakers repeat constructions frequently in dialogue. Due to their peculiar information-theoretic properties,
    repetitions can be thought of as a strategy for cost-effective communication. In this study, we focus on the repetition of
    lexicalised constructions -- i.e., recurring multi-word units -- in English open-domain spoken dialogues. We hypothesise
    that speakers use construction repetition to mitigate information rate, leading to an overall decrease in utterance information
    content over the course of a dialogue. We conduct a quantitative analysis, measuring the information content of constructions
    and that of their containing utterances, estimating information content with an adaptive neural language model. We observe
    that construction usage lowers the information content of utterances. This facilitating effect (i) increases throughout dialogues,
    (ii) is boosted by repetition, (iii) grows as a function of repetition frequency and density, and (iv) is stronger for repetitions
    of referential constructions."
}

@Article{sinclair-etal-2022-tacl,
  author = 	 "Arabella Sinclair and Jaap Jumelet and Willem Zuidema and Raquel Fern\'andez",
  title = 	 "Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations",
  journal = 	 "Transactions of the Association for Computational Linguistics (TACL)",
  year = "2022",
  url = "https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00504/113019/Structural-Persistence-in-Language-Models-Priming",
  abstract = "We investigate the extent to which modern, neural language models are susceptible to structural priming,
  the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up  sentence.
  We explore how priming can be used to study the potential of these models to learn abstract structural information,
  which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce
  a novel metric and release PRIME-LM, a large corpus where we control for various linguistic factors which interact with priming strength.
  We find that Transformer models indeed show evidence of structural priming, but also that the generalisations they learned are to
  some extent modulated by semantic information. Our experiments also show that the representations acquired by the models
  may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information.
  More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities
  of language models and opens the door to future priming-based investigations that probe the model's internal states."
}

@inproceedings{ryb-etal-2022-analog,
    title = "{A}na{L}og: Testing Analytical and Deductive Logic Learnability in Language Models",
    author = "Ryb, Samuel  and
      Giulianelli, Mario  and
      Sinclair, Arabella  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 11th Joint Conference on Lexical and Computational Semantics",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.starsem-1.5",
    pages = "55--68",
    abstract = "We investigate the extent to which pre-trained language models acquire analytical and deductive logical reasoning
    capabilities as a side effect of learning word prediction. We present AnaLog, a natural language inference task designed to
    probe models for these capabilities, controlling for different invalid heuristics the models may adopt instead of learning the
    desired generalisations. We test four languagemodels on AnaLog, finding that they have all learned, to a different extent,
    to encode information that is predictive of entailment beyond shallow heuristics such as lexical overlap and grammaticality.
    We closely analyse the best performing language model and show that while it performs more consistently than other language
    models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the
    realisation of logical statements."
}

@inproceedings{takmaz-etal-2022-cmcl,
    title = "Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via {CLIP}",
    author = "Takmaz, Ece  and Pezzelle, Sandro  and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.cmcl-1.4",
    pages = "36--42",
    abstract = "In this work, we use a transformer-based pre-trained multimodal model, CLIP,
    to shed light on the mechanisms employed by human speakers when referring to visual entities.
    In particular, we use CLIP to quantify the degree of descriptiveness (how well an utterance
    describes an image in isolation) and discriminativeness (to what extent an utterance is effective
    in picking out a single image among similar images) of human referring utterances within multimodal
    dialogues. Overall, our results show that utterances become less descriptive over time while their
    discriminativeness remains unchanged. Through analysis, we propose that this trend could be due to
    participants relying on the previous mentions in the dialogue history, as well as being able to
    distill the most discriminative information from the visual context. In general, our study opens
    up the possibility of using this and similar models to quantify patterns in human data and shed
    light on the underlying cognitive mechanisms."
}

@inproceedings{takmaz-2022-team,
    title = "Team {DMG} at {CMCL} 2022 Shared Task: Transformer Adapters for the Multi- and Cross-Lingual Prediction of Human Reading Behavior",
    author = "Takmaz, Ece",
    booktitle = "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics",
    month = may,
    year = "2022",
    bibbase_note = {Best Shared Task Paper Award},
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.cmcl-1.16",
    pages = "136--144",
    abstract = "In this paper, we present the details of our approaches that attained the second place
    in the shared task of the ACL 2022 Cognitive Modeling and Computational Linguistics Workshop.
    The shared task is focused on multi- and cross-lingual prediction of eye movement features in
    human reading behavior, which could provide valuable information regarding language processing.
    To this end, we train {`}adapters{'} inserted into the layers of frozen transformer-based pretrained
    language models. We find that multilingual models equipped with adapters perform well in predicting
    eye-tracking features. Our results suggest that utilizing language- and task-specific adapters is
    beneficial and translating test sets into similar languages that exist in the training set could
    help with zero-shot transferability in the prediction of human reading behavior."
}

@inproceedings{giulianelli-etal-2022-fire,
    title = "Do Not Fire the Linguist: Grammatical Profiles Help Language Models Detect Semantic Change",
    author = "Giulianelli, Mario  and Kutuzov, Andrey  and Pivovarova, Lidia",
    booktitle = "Proceedings of the 3rd Workshop on Computational Approaches to Historical Language Change",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.lchange-1.6",
    pages = "54--67",
    abstract = "Morphological and syntactic changes in word usage {---} as captured, e.g., by
    grammatical profiles {---} have been shown to be good predictors of a word{'}s meaning change.
    In this work, we explore whether large pre-trained contextualised language models, a common tool
    for lexical semantic change detection, are sensitive to such morphosyntactic changes. To this end,
    we first compare the performance of grammatical profiles against that of a multilingual neural
    language model (XLM-R) on 10 datasets, covering 7 languages, and then combine the two approaches
    in ensembles to assess their complementarity. Our results show that ensembling grammatical
    profiles with XLM-R improves semantic change detection performance for most datasets and languages.
    This indicates that language models do not fully cover the fine-grained morphological and syntactic
    signals that are explicitly represented in grammatical profiles. An interesting exception are the
    test sets where the time spans under analysis are much longer than the time gap between them
    (for example, century-long spans with a one-year gap between them). Morphosyntactic change is
    slow so grammatical profiles do not detect in such cases. In contrast, language models, thanks
    to their access to lexical information, are able to detect fast topical changes."
}


@Article{pezzelle-etal-2021-tacl,
  author = 	 "Sandro Pezzelle and Ece Takmaz and Raquel Fern\'andez",
  title = 	 "Word Representation Learning in Multimodal Pre-Trained Transformers: An Intrinsic Evaluation",
  journal = 	 "Transactions of the Association for Computational Linguistics (TACL)",
  year = "2021",
  url = "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00443/1979754/tacl_a_00443.pdf",
  url_github = "https://github.com/sandropezzelle/multimodal-evaluation",
  abstract = "This study carries out a systematic /intrinsic/ evaluation of the semantic representations
	learned by state-of-the-art pre-trained multimodal Transformers.
	These representations are claimed to be task-agnostic and shown to help on many downstream language-and-vision tasks.
	However, the extent to which they align with human semantic intuitions remains unclear.
	We experiment with various models and obtain /static/ word representations from the /contextualized/ ones they learn.
	We then evaluate them against the semantic judgements provided by human speakers. In line with previous evidence,
	we observe a generalized advantage of multimodal representations over language-only ones on concrete word pairs,
  but not on abstract ones. On the one hand, this confirms the effectiveness of these models to
	align language and vision, which results in better semantic representations for concepts that are /grounded/ in images.
	On the other hand, models are shown to follow different representation learning patterns, which
	sheds some light on /how/ and /when/ they perform multimodal integration."
}


@inproceedings{jansen-etal-2021-detecting,
    title = "Detecting Age-Related Linguistic Patterns in Dialogue: Toward Adaptive Conversational Systems",
    author = "Jansen, Lennert and Sinclair, Arabella and van der Goot, Margot J. and Fern{\'a}ndez, Raquel and Pezzelle, Sandro",
    booktitle = "Proceedings of the Eighth Italian Conference on Computational Linguistics (CLiC-it)",
    year = "2021",
    url = "http://ceur-ws.org/Vol-3033/paper47.pdf",
    url_github = "https://github.com/ lennertjansen/detecting-age-in-dialogue",
    abstract = "This work explores an important dimension of variation in the language used by dialogue
    participants: their age. While previous work showed differences at various linguistic levels between
    age groups when experimenting with written discourse data (e.g., blog posts), previous work on dialogue
    has largely been limited to acoustic information related to voice and prosody. Detecting fine-grained
    linguistic properties of human dialogues is of crucial importance for developing AI- based conversational
    systems which are able to adapt to their human interlocutors. We therefore investigate whether, and to
    what extent, current text-based NLP models can detect such linguistic differences, and what the features
    driving their predictions are. We show that models achieve a fairly good performance on age- group prediction,
    though the task appears to be more challenging compared to discourse. Through in-depth analysis of the
    best models’ errors and the most predictive cues, we show that, in dialogue, differences among age groups
    mostly concern stylistic and lexical choices. We believe these findings can inform future work on developing
    controlled generation models for adaptive conversational systems."
}

@inproceedings{giulianelli-etal-2021-information,
    title = "Is Information Density Uniform in Task-Oriented Dialogues?",
    author = "Giulianelli, Mario and Sinclair, Arabella and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.emnlp-main.652/",
    url_github = "https://github.com/dmg-illc/uid-dialogue",
    abstract = "The Uniform Information Density principle states that speakers plan their utterances to reduce fluctuations
    in the density of the information transmitted. In this paper, we test whether, and within which contextual units this
    principle holds in task-oriented dialogues. We show that there is evidence supporting the principle in written dialogues
    where participants play a cooperative reference game as well as in spoken dialogues involving instruction giving and following.
    Our study underlines the importance of identifying the relevant contextual components, showing that information content
    increases particularly within topically and referentially related contextual units."
}

@inproceedings{giulianelli-fernandez-2021-analysing,
    title = "Analysing Human Strategies of Information Transmission as a Function of Discourse Context",
    author = "Giulianelli, Mario and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.conll-1.50/",
    url_github = "https://github.com/dmg-illc/uid-dialogue",
    abstract = "Speakers are thought to use rational information transmission strategies for efficient communication.
    Previous work analysing these strategies in sentence production has failed to take into account how the information
    content of sentences varies as a function of the available discourse context.
    In this study, we estimate sentence information content within discourse context.
    We find that speakers transmit information at a stable rate---i.e., rationally---in English newspaper articles
    but that this rate decreases in spoken open domain and written task-oriented dialogues.
    We also observe that speakers' choices are not oriented towards local uniformity of information,
    which is another hypothesised rational strategy. We suggest that a more faithful model of communication
    should explicitly include production costs and goal-oriented rewards."
}

@inproceedings{sinclair-fernandez-2021-construction,
    title = "Construction coordination in first and second language acquisition",
    author = "Sinclair, Arabella  and Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 25th Workshop on the Semantics and Pragmatics of Dialogue",
    year = "2021",
    address = "Potsdam, Germany",
    publisher = "SEMDIAL",
    url = "http://semdial.org/anthology/Z21-Sinclair_semdial_0018.pdf",
    abstract = "Repetition of linguistic forms is a pervasive coordination mechanism in interactive language use.
    In this paper, we investigate patterns of cross-participant repetition in dialogues where participants have
    different levels of linguistic ability. Achieving a better understanding of these patterns can not only shed
    light on how humans coordinate in conversation, but may also contribute to developing more natural and effective
    dialogue agents in education contexts related to language learning. Our approach is novel in several respects:
    We focus on multi-word constructions at the lexical and morphosyntactic level, consider both first and second
    acquisition dialogue, and contrast these setups with adult native conversation. The results of our study show
    that language acquisition scenarios are characterised by richer inventories of shared constructions but lower
    usage rates than fluent adult dialogues, and that shared construction use evolves as the linguistic ability
    of the learners increases, arguably leading to a process of routinisation."
}

@inproceedings{giulianelli-etal-2021-profiling,
    title = "Grammatical Profiling for Semantic Change Detection",
    author = "Giulianelli, Mario and Kutuzov, Andrey and Pivovarova, Lidia",
    booktitle = "Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL)",
    year = "2021",
    publisher = "Association for Computational Linguistics.",
    url = "https://aclanthology.org/2021.conll-1.33/",
    url_GitHub = "https://github.com/glnmario/semchange-profiling",
    abstract = "Semantics, morphology and syntax are strongly interdependent. However, the majority of computational methods for semantic change detection use distributional word representations which encode mostly semantics. We investigate an alternative method, grammatical profiling, based entirely on changes in the morphosyntactic behaviour of words. We demonstrate that it can be used for semantic change detection and even outperforms some distributional semantic methods. We present an in-depth qualitative and quantitative analysis of the predictions made by our grammatical profiling system, showing that they are plausible and interpretable."
}

@inproceedings{parfenova-etal-2021-probing,
    title = "Probing Cross-Modal Representations in Multi-Step Relational Reasoning",
    author = "Parfenova, Iuliia  and
      Elliott, Desmond  and
      Fern{\'a}ndez, Raquel  and
      Pezzelle, Sandro",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.16.pdf",
    doi = "10.18653/v1/2021.repl4nlp-1.16",
    url_GitHub = "https://github.com/jig-san/multi-step-size-reasoning",
    pages = "152--162",
    abstract = "We investigate the representations learned by vision and language models in tasks that require relational reasoning. Focusing on the problem of assessing the relative size of objects in abstract visual contexts, we analyse both one-step and two-step reasoning. For the latter, we construct a new dataset of three-image scenes and define a task that requires reasoning at the level of the individual images and across images in a scene. We probe the learned model representations using diagnostic classifiers. Our experiments show that pretrained multimodal transformer-based architectures can perform higher-level relational reasoning, and are able to learn representations for novel tasks and data that are very different from what was seen in pretraining.",
}

@inproceedings{noble-etal-2021-semantic,
    title = "Semantic shift in social networks",
    author = "Noble, Bill  and
      Sayeed, Asad  and
      Fern{\'a}ndez, Raquel  and
      Larsson, Staffan",
    booktitle = "Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.starsem-1.3.pdf",
    doi = "10.18653/v1/2021.starsem-1.3",
    url_GitHub = "https://github.com/GU-CLASP/semantic-shift-in-social-networks",
    pages = "26--37",
    abstract = "Just as the meaning of words is tied to the communities in which they are used, so too is semantic change. But how does lexical semantic change manifest differently across different communities? In this work, we investigate the relationship between community structure and semantic change in 45 communities from the social media website Reddit. We use distributional methods to quantify lexical semantic change and induce a social network on communities, based on interactions between members. We explore the relationship between semantic change and the clustering coefficient of a community{'}s social network graph, as well as community size and stability. While none of these factors are found to be significant on their own, we report a significant effect of their three-way interaction. We also report on significant word-level effects of frequency and change in frequency, which replicate previous findings.",
}

@inproceedings{takmaz-etal-2020-refer,
    title = "{R}efer, {R}euse, {R}educe: {G}enerating {S}ubsequent {R}eferences in {V}isual and {C}onversational {C}ontexts",
    author = "Takmaz, Ece  and
      Giulianelli, Mario  and
      Pezzelle, Sandro  and
      Sinclair, Arabella  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.353.pdf",
    url_dataset = "https://dmg-photobook.github.io",
    doi = "10.18653/v1/2020.emnlp-main.353",
    pages = "4350--4368",
    abstract = "Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness. Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions. In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue. We propose a generation model that produces referring utterances grounded in both the visual and the conversational context. To assess the referring effectiveness of its output, we also implement a reference resolution system. Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.",
}

@inproceedings{takmaz-etal-2020-generating,
    title = "{G}enerating {I}mage {D}escriptions via {S}equential {C}ross-{M}odal {A}lignment {G}uided by {H}uman {G}aze",
    author = "Takmaz, Ece  and
      Pezzelle, Sandro  and
      Beinborn, Lisa  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.377.pdf",
    url_GitHub = "https://github.com/dmg-illc/didec-seq-gen",
    doi = "10.18653/v1/2020.emnlp-main.353",
    pages = "4664--4677",
    abstract = "When speakers describe an image, they tend to look at objects before mentioning them. In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally. We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production. In particular, we propose the first approach to image description generation where visual processing is modelled sequentially. Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production. We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural{---}particularly when gaze is encoded with a dedicated recurrent component.",
}


@inproceedings{giulianelli-etal-2020-analysing,
    title = "Analysing Lexical Semantic Change with Contextualised Word Representations",
    author = "Giulianelli, Mario  and
      Del Tredici, Marco  and
      Fern{\'a}ndez, Raquel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.365.pdf",
    url_Dataset = {https://doi.org/10.5281/zenodo.3773250},
    doi = "10.18653/v1/2020.acl-main.365",
    pages = "3960--3973",
    abstract = "This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.",
}

@inproceedings{kutuzov-giulianelli-2020-change,
    title = "{U}i{O}-{U}v{A} at {S}em{E}val-2020 Task 1: Contextualised Embeddings for Lexical Semantic Change Detection",
    author = "Kutuzov, Andrey  and
      Giulianelli, Mario",
    booktitle = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://aclanthology.org/2020.semeval-1.14",
    url_GitHub = "https://github.com/akutuzov/semeval2020",
    pages = "126--134",
    abstract = "We apply contextualised word embeddings to lexical semantic change detection in the SemEval-2020 Shared Task 1. This paper focuses on Subtask 2, ranking words by the degree of their semantic drift over time. We analyse the performance of two contextualising architectures (BERT and ELMo) and three change detection algorithms. We find that the most effective algorithms rely on the cosine similarity between averaged token embeddings and the pairwise distances between token embeddings. They outperform strong baselines by a large margin (in the post-evaluation phase, we have the best Subtask 2 submission for SemEval-2020 Task 1), but interestingly, the choice of a particular algorithm depends on the distribution of gold scores in the test set.",
}


@inproceedings{gualdoni-etal:2020:clicit,
  author={Eleonora Gualdoni and Raffaella Bernardi and Raquel Fern\'andez and Sandro Pezzelle},
  title={Grounded and Ungrounded Referring Expressions in Human Dialogues: Language Mirrors Different Grounding Conditions},
  booktitle={Proceedings of the 7th Italian Conference on Computational Linguistics (CLiC-it)},
  year={2020},
  url={http://ceur-ws.org/Vol-2769/paper_38.pdf}
}
